# DSA CHEAT SHEET - Complete Reference

## Table of Contents
1. Big-O Notation & Complexity Analysis
2. Data Structures Overview
3. Linked Lists
4. Stacks & Queues
5. Trees
6. Graphs
7. Hashing
8. Sorting Algorithms
9. Search Algorithms
10. Recursion

---

## 1. BIG-O NOTATION & COMPLEXITY ANALYSIS

### What is Big-O?
Big-O notation describes the **upper bound** on algorithm complexity (worst-case scenario). It represents how the algorithm's performance scales with input size.

### Complexity Hierarchy (Fastest to Slowest)
```
O(1) < O(log n) < O(n) < O(n log n) < O(n²) < O(n³) < O(2ⁿ) < O(n!)
Constant < Logarithmic < Linear < Linearithmic < Quadratic < Cubic < Exponential < Factorial
```

### Best, Average, Worst Cases
- **Best Case (Ω - Omega)**: Minimum time/space needed
- **Average Case (Θ - Theta)**: Expected performance
- **Worst Case (O - Big O)**: Maximum time/space needed

### Examples
| Big-O | Example | Meaning |
|-------|---------|---------|
| O(1) | Hash table lookup | Constant - independent of input |
| O(log n) | Binary search | Logarithmic - halves input each step |
| O(n) | Linear search | Linear - scales with input size |
| O(n log n) | Merge sort | Linearithmic - divide & conquer |
| O(n²) | Bubble sort | Quadratic - nested loops |
| O(2ⁿ) | Fibonacci recursion | Exponential - branches exponentially |

---

## 2. DATA STRUCTURES OVERVIEW

### Linear Data Structures

| Structure | Access | Search | Insert | Delete | Space | Use Case |
|-----------|--------|--------|--------|--------|-------|----------|
| Array | O(1) | O(n) | O(n) | O(n) | O(n) | Fast access, fixed size |
| Singly LL | O(n) | O(n) | O(1)* | O(1)* | O(n) | Dynamic, one direction |
| Doubly LL | O(n) | O(n) | O(1)* | O(1)* | O(n) | Bidirectional traversal |
| Stack | O(n) | O(n) | O(1) | O(1) | O(n) | LIFO - undo/redo |
| Queue | O(n) | O(n) | O(1) | O(1) | O(n) | FIFO - scheduling |
| Hash Table | O(1)^ | O(1)^ | O(1)^ | O(1)^ | O(n) | Fast lookup |

*At known position; ^Average case

### Non-Linear Data Structures

| Structure | Insert | Delete | Search | Space | Use Case |
|-----------|--------|--------|--------|-------|----------|
| BST | O(log n) | O(log n) | O(log n) | O(n) | Sorted data |
| AVL Tree | O(log n) | O(log n) | O(log n) | O(n) | Self-balanced |
| Heap | O(log n) | O(log n) | O(n) | O(n) | Priority queue |
| Graph | - | - | O(V+E) | O(V+E) | Networks, relations |

---

## 3. LINKED LISTS

### Singly Linked List
```
Node Structure: [Data | Next Pointer]
                  ↓         ↓
                [10 | ●] → [20 | ●] → [30 | NULL]
```
- **Traversal**: One direction only (forward)
- **Insert at head**: O(1)
- **Insert at tail**: O(n) (without tail pointer)
- **Delete**: O(n) (need to find previous)
- **Space**: O(n)

### Doubly Linked List
```
Node Structure: [Prev | Data | Next]
                  ↑    ↓      ↓
         NULL ← [10 | ● | ●] ↔ [20 | ● | ●] ↔ [30 | NULL]
```
- **Traversal**: Both directions
- **Insert/Delete from either end**: O(1)
- **Space**: O(n) (extra pointer per node)
- **Use**: Browser history, undo/redo

### Circular Linked List
```
Last node points to First node
[10] → [20] → [30] → [10] (loops)
```
- **No NULL terminator**
- **Circular queue implementation**
- **Useful for**: Round-robin scheduling, circular buffers

### Operations Complexity
| Operation | Singly LL | Doubly LL | Circular LL |
|-----------|-----------|-----------|------------|
| Access i-th | O(n) | O(n) | O(n) |
| Insert at head | O(1) | O(1) | O(1) |
| Insert at tail | O(n) | O(1) | O(1) |
| Delete head | O(1) | O(1) | O(1) |
| Delete tail | O(n) | O(1) | O(1) |

---

## 4. STACKS & QUEUES

### Stack (LIFO - Last In First Out)
```
Push: 1, 2, 3 → [3, 2, 1] ← Pop
        Top ↑
```
- **Operations**: Push O(1), Pop O(1), Peek O(1)
- **Applications**: Function call stack, undo/redo, expression evaluation, DFS
- **Implementation**: Array or Linked List

### Queue (FIFO - First In First Out)
```
Enqueue: 1, 2, 3
Front: 1 ← Dequeue first
Rear: 3 ← Enqueue here
```
- **Operations**: Enqueue O(1), Dequeue O(1), Peek O(1)
- **Applications**: Scheduling, BFS, buffering

### Circular Queue
- **Eliminates wasted space** in linear queue
- Rear pointer wraps around after reaching end
- **Operations**: O(1) for all operations
- **Advantage**: Better memory utilization

### Priority Queue
- **Elements removed by priority** (not FIFO)
- **Implementation**: Heap (best: O(log n) insert/delete)
- **Use**: Dijkstra's algorithm, Huffman coding

| Operation | Stack | Queue | Circular Q | Priority Q |
|-----------|-------|-------|-----------|-----------|
| Insert | O(1) | O(1) | O(1) | O(log n) |
| Delete | O(1) | O(1) | O(1) | O(log n) |
| Peek | O(1) | O(1) | O(1) | O(1) |

---

## 5. TREES

### Tree Terminology
- **Root**: Top node
- **Leaf**: Node with no children
- **Height**: Longest path from node to leaf
- **Depth**: Distance from root to node
- **Subtree**: Node + its descendants

### Binary Tree
- **Each node has at most 2 children**
- Max nodes at level k: 2^k
- Max nodes in tree of height h: 2^(h+1) - 1

#### Tree Traversals (for Binary Tree)
```
       1
      / \
     2   3
    / \
   4   5

Pre-order (Root-Left-Right):   1, 2, 4, 5, 3
In-order (Left-Root-Right):    4, 2, 5, 1, 3  [Sorted for BST]
Post-order (Left-Right-Root):  4, 5, 2, 3, 1
Level-order (BFS):             1, 2, 3, 4, 5
```

### Binary Search Tree (BST)
- **Left child < Parent < Right child**
- **Search, Insert, Delete**: O(log n) average, O(n) worst (unbalanced)
- **In-order traversal gives sorted sequence**

### AVL Tree (Self-Balancing BST)
- **Balance factor**: Height(Left) - Height(Right) ∈ {-1, 0, 1}
- **All operations**: O(log n) guaranteed (via rotations)
- **Most balanced, slightly slower insertion**

### Red-Black Tree
- **Color-based balancing** (Red/Black nodes)
- **All operations**: O(log n)
- **Less strict balance** than AVL (faster insertion)

### Heap
**Complete Binary Tree** where parent ≥ all children (Max Heap) or parent ≤ all children (Min Heap)
- **Insert**: O(log n)
- **Delete min/max**: O(log n)
- **Build heap**: O(n)
- **Use**: Priority queues, heap sort

| Tree Type | Balanced | Insert | Delete | Search | Use Case |
|-----------|----------|--------|--------|--------|----------|
| BST | No | O(log n) | O(log n) | O(log n) | Sorted data |
| AVL | Yes | O(log n) | O(log n) | O(log n) | Strict balance needed |
| RB-Tree | Yes* | O(log n) | O(log n) | O(log n) | Less strict balance |
| Heap | Partial | O(log n) | O(log n) | O(n) | Priority queue |

*Loosely balanced

---

## 6. GRAPHS

### Graph Types
**Undirected**: Edges have no direction (A-B same as B-A)
**Directed**: Edges have direction (A→B ≠ B→A)
**Weighted**: Edges have costs/weights
**Unweighted**: All edges equal

### Representation
**Adjacency Matrix**: O(V²) space, O(1) edge lookup
**Adjacency List**: O(V+E) space, O(E/V) average edge lookup

### Graph Traversal
**BFS (Breadth-First Search)**
- Uses Queue
- Time: O(V+E)
- Find shortest path in unweighted graph

**DFS (Depth-First Search)**
- Uses Stack/Recursion
- Time: O(V+E)
- Detect cycles, topological sort

### Common Algorithms
| Algorithm | Time | Use |
|-----------|------|-----|
| BFS | O(V+E) | Shortest path (unweighted) |
| DFS | O(V+E) | Cycles, connectivity |
| Dijkstra | O((V+E)log V) | Shortest path (weighted, positive) |
| Floyd-Warshall | O(V³) | All-pairs shortest path |
| Kruskal | O(E log E) | Minimum spanning tree |
| Prim | O(V²) or O(E log V) | Minimum spanning tree |
| Topological Sort | O(V+E) | DAG ordering |

---

## 7. HASHING

### Hash Table Basics
- **Maps keys to values** using hash function
- **Hash Function**: Converts key → array index
- **Collision Resolution**: 
  - Chaining: Store collisions in linked list
  - Open Addressing: Find another empty slot (linear probing, quadratic, double hashing)

### Operations
| Operation | Average | Worst |
|-----------|---------|-------|
| Insert | O(1) | O(n) |
| Delete | O(1) | O(n) |
| Search | O(1) | O(n) |

*Worst case when many collisions (bad hash function)*

### Load Factor
λ = (Number of entries) / (Hash table size)
- Typical threshold: 0.75 (resize when exceeded)

### Applications
- Dictionary/Map implementation
- Caching (memoization)
- Deduplication
- Frequency counting

---

## 8. SORTING ALGORITHMS

### Comparison-Based Sorts

| Algorithm | Best | Average | Worst | Space | Stable | In-place |
|-----------|------|---------|-------|-------|--------|----------|
| Bubble Sort | O(n) | O(n²) | O(n²) | O(1) | Yes | Yes |
| Selection Sort | O(n²) | O(n²) | O(n²) | O(1) | No | Yes |
| Insertion Sort | O(n) | O(n²) | O(n²) | O(1) | Yes | Yes |
| Merge Sort | O(n log n) | O(n log n) | O(n log n) | O(n) | Yes | No |
| Quick Sort | O(n log n) | O(n log n) | O(n²) | O(log n) | No* | Yes |
| Heap Sort | O(n log n) | O(n log n) | O(n log n) | O(1) | No | Yes |
| Shell Sort | - | O(n^1.3) | O(n²) | O(1) | No | Yes |

*Not stable in standard implementation

### Non-Comparison Sorts (Linear Time)

| Algorithm | Time | Space | Use |
|-----------|------|-------|-----|
| Counting Sort | O(n+k) | O(k) | Small range integers |
| Radix Sort | O(d(n+k)) | O(n+k) | Digits/characters |
| Bucket Sort | O(n+k) | O(n) | Uniform distribution |

### When to Use?
- **Small data**: Insertion Sort (simple, adaptive)
- **General purpose**: Merge Sort (stable, O(n log n) guaranteed)
- **Average performance**: Quick Sort (practical, cache-friendly)
- **No extra space**: Heap Sort (in-place, O(n log n))
- **Nearly sorted**: Insertion Sort, Shell Sort
- **Integers in range**: Counting/Radix/Bucket Sort

---

## 9. SEARCH ALGORITHMS

### Linear Search
```
Time: O(n)
Space: O(1)
Use: Unsorted array, small data
```

### Binary Search
```
Time: O(log n)
Space: O(1) iterative, O(log n) recursive
Prerequisite: Sorted array
```

### Hash-based Search
```
Time: O(1) average, O(n) worst
Space: O(n)
Use: Dictionary lookup
```

---

## 10. RECURSION

### Definition
Function calling itself to solve smaller subproblems.

### Essential Components
1. **Base Case**: Stopping condition (no recursion)
2. **Recursive Case**: Function calls itself with smaller problem
3. **Progress**: Moving toward base case

### Recursion vs Iteration
| Aspect | Recursion | Iteration |
|--------|-----------|-----------|
| Code | More readable | More efficient |
| Space | Extra (call stack) | Minimal |
| Time | Overhead of function calls | Direct execution |
| Stack Overflow | Risk if deep | No risk |

### Disadvantages of Recursion
- **Extra memory**: Function call stack overhead
- **Stack overflow**: Deep recursion → stack limit
- **Performance**: Function call overhead

### Types of Recursion
**Direct**: Function f calls f directly
**Indirect**: Function f calls g, which calls f
**Tail Recursion**: Last operation is recursive call (compiler can optimize)

### Common Examples
- Factorial: O(n) time, O(n) space
- Fibonacci: O(2^n) time (exponential!)
- Tree traversal: O(n) time, O(h) space
- Binary search: O(log n) time, O(log n) space

---

## QUICK REFERENCE - COMPLEXITY CHART

### Time Complexities by Algorithm
```
O(1): Constant - Hash lookup, array access
O(log n): Logarithmic - Binary search, balanced tree operations
O(n): Linear - Linear search, array traversal
O(n log n): Linearithmic - Merge sort, heap sort, quick sort (avg)
O(n²): Quadratic - Bubble sort, selection sort, insertion sort
O(n³): Cubic - Triple nested loops
O(2ⁿ): Exponential - Recursive Fibonacci, subset generation
O(n!): Factorial - Permutations, naive travelling salesman
```

### Space Complexities
```
O(1): Constant space
O(log n): Recursive call stack
O(n): Proportional to input
O(n²): 2D array/matrix
```

---

## QUIZ ANSWERS WITH EXPLANATIONS

**Q1: What does Big-O notation describe?**
- **Answer: B) Worst-case complexity**
- Big-O specifically describes upper bound (worst case)

**Q2: Which has lowest time complexity?**
- **Answer: D) O(log n)**
- Order: O(1) < O(log n) < O(n) < O(n²)

**Q3: If algorithm runs in O(1)?**
- **Answer: C) Constant time regardless of input size**
- O(1) means no dependence on input size

**Q4: ADT defines?**
- **Answer: C) Data and operations, not implementation**
- ADT specifies WHAT, not HOW

**Q5: LIFO structure?**
- **Answer: B) Stack**
- Last In First Out = Stack (FIFO = Queue)

**Q6: Queue insertion/deletion occur at?**
- **Answer: C) Rear and Front respectively**
- Enqueue at rear, dequeue from front

**Q7: Circular queue used to?**
- **Answer: B) Avoid memory wastage**
- Wrapping reuses space, no gaps

**Q8: Singly linked list node contains?**
- **Answer: C) Data and one link**
- Single pointer to next node

**Q9: Bidirectional traversal?**
- **Answer: C) Doubly**
- Doubly linked list has prev and next pointers

**Q10: Circular linked list?**
- **Answer: C) Last node points to first**
- Creates loop structure

**Q11: Recursion must have?**
- **Answer: B) Base condition**
- Otherwise infinite recursion/stack overflow

**Q12: Direct recursion means?**
- **Answer: B) Function calls itself**
- f → f (directly), not f → g → f (indirect)

**Q13: Disadvantage of recursion?**
- **Answer: B) Extra memory usage**
- Call stack uses significant memory

**Q14: At most two children?**
- **Answer: B) Binary Tree**
- Definition of binary tree

**Q15: In-order traversal?**
- **Answer: B) Left-Root-Right**
- Gives sorted output for BST

**Q16: AVL tree is?**
- **Answer: B) Balanced BST**
- Self-balancing ensures O(log n) operations

**Q17: Binary search time complexity?**
- **Answer: C) O(log n)**
- Halves search space each iteration

**Q18: Stable sorting algorithm?**
- **Answer: C) Merge Sort**
- Maintains relative order of equal elements

**Q19: Quick sort worst case?**
- **Answer: C) O(n²)**
- When pivot always smallest/largest (bad partitioning)

**Q20: Hashing mainly used for?**
- **Answer: B) Searching**
- O(1) average lookup time

---

## ADDITIONAL TIPS

### Interview Preparation
1. Know Big-O analysis for all common data structures
2. Understand when to use each data structure
3. Implement traversals for trees/graphs
4. Practice sorting algorithms from scratch
5. Recognize problem patterns (use hash for duplicates, trees for hierarchy, etc.)

### Common Mistakes
- Confusing time and space complexity
- Not considering worst-case scenario
- Using unstable sort when stability matters
- Not knowing trade-offs between data structures
- Forgetting base case in recursion

### Optimization Strategies
- Trade space for time (hash tables, memoization)
- Use appropriate data structure for problem
- Divide and conquer for scalability
- Greedy algorithms when applicable
- Dynamic programming for overlapping subproblems